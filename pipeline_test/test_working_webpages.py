#!/usr/bin/env python3
"""
Test Pipeline with Working Webpages

This test uses webpages that we know work to verify
web scraping and data extraction capabilities.
"""

import sys
import json
from pathlib import Path
from typing import Dict, Any

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from utils.config import get_config
from utils.web_tools import (
    search_web,
    scrape_webpage,
    extract_financial_data
)


def test_working_webpages():
    """Test with webpages that we know work."""
    print("\nğŸŒ Testing Working Webpages...")
    
    try:
        # Test with working URLs
        working_urls = [
            "https://www.hacienda.gov.py/",
            "https://www.google.com/",
            "https://www.wikipedia.org/"
        ]
        
        successful_scrapes = 0
        
        for url in working_urls:
            print(f"      Testing: {url}")
            result = scrape_webpage(url)
            
            if result.get("content"):
                print(f"         âœ… Scraped successfully")
                print(f"            Title: {result.get('title', 'No title')}")
                print(f"            Content length: {len(result.get('content', ''))} chars")
                successful_scrapes += 1
            else:
                print(f"         âŒ Failed to scrape: {result.get('metadata', {}).get('error', 'Unknown error')}")
        
        if successful_scrapes > 0:
            print(f"  âœ… Working webpages test: {successful_scrapes}/{len(working_urls)} successful")
            return True
        else:
            print("  âŒ Working webpages test failed")
            return False
        
    except Exception as e:
        print(f"  âŒ Working webpages test failed: {str(e)}")
        return False


def test_financial_data_extraction_working():
    """Test financial data extraction from working webpages."""
    print("\nğŸ’° Testing Financial Data Extraction (Working Pages)...")
    
    try:
        # Test with Ministerio de Hacienda (we know it works)
        test_url = "https://www.hacienda.gov.py/"
        result = extract_financial_data(test_url)
        
        if result and "error" not in result:
            print("  âœ… Financial data extraction working")
            print(f"      URL: {result.get('url', 'No URL')}")
            print(f"      Extracted data length: {len(result.get('extracted_data', ''))} chars")
            print(f"      Raw content preview: {result.get('raw_content', '')[:100]}...")
            return True
        else:
            print(f"  âŒ Financial data extraction failed: {result.get('error', 'Unknown error')}")
            return False
        
    except Exception as e:
        print(f"  âŒ Financial data extraction failed: {str(e)}")
        return False


def test_paraguayan_search():
    """Test search for Paraguayan content."""
    print("\nğŸ‡µğŸ‡¾ Testing Paraguayan Content Search...")
    
    try:
        # Test search for Paraguayan government content
        query = "site:gov.py paraguay economÃ­a"
        results = search_web(query, num_results=3)
        
        if results:
            print("  âœ… Paraguayan content search working")
            print(f"      Found {len(results)} results")
            for i, result in enumerate(results[:2]):
                print(f"      {i+1}. {result.get('title', 'No title')}")
                print(f"         URL: {result.get('url', 'No URL')}")
                print(f"         Snippet: {result.get('snippet', '')[:100]}...")
            return True
        else:
            print("  âŒ Paraguayan content search failed - no results")
            return False
        
    except Exception as e:
        print(f"  âŒ Paraguayan content search failed: {str(e)}")
        return False


def test_content_analysis():
    """Test content analysis with scraped data."""
    print("\nğŸ“Š Testing Content Analysis...")
    
    try:
        from langchain_google_genai import ChatGoogleGenerativeAI
        config = get_config()
        model_config = config.get_model_config()
        
        # Create LLM
        llm = ChatGoogleGenerativeAI(
            model=model_config["llm_model"],
            google_api_key=model_config["api_key"],
            temperature=model_config["temperature"],
            max_tokens=model_config["max_output_tokens"]
        )
        
        # Scrape a working webpage
        scraped = scrape_webpage("https://www.hacienda.gov.py/")
        
        if scraped.get("content"):
            # Analyze the content
            prompt = f"""
            Analiza este contenido web del Ministerio de Hacienda de Paraguay:
            
            TÃ­tulo: {scraped.get('title', '')}
            Contenido: {scraped.get('content', '')[:1000]}
            
            Identifica:
            1. Â¿Es una pÃ¡gina oficial del gobierno paraguayo?
            2. Â¿QuÃ© tipo de informaciÃ³n contiene?
            3. Â¿Hay datos financieros o econÃ³micos?
            
            Responde en formato JSON.
            """
            
            response = llm.invoke(prompt)
            
            print("  âœ… Content analysis working")
            print(f"      Analysis: {response.content[:200]}...")
            return True
        else:
            print("  âŒ Content analysis failed - no content to analyze")
            return False
        
    except Exception as e:
        print(f"  âŒ Content analysis failed: {str(e)}")
        return False


def main():
    """Run tests with working webpages."""
    print("ğŸš€ Testing Pipeline with Working Webpages")
    print("=" * 60)
    print("ğŸ“Š Testing web scraping with known working pages")
    print("ğŸ”’ No database writes - Context-efficient testing only")
    print("=" * 60)
    
    # Test each component
    results = []
    
    results.append(test_working_webpages())
    results.append(test_financial_data_extraction_working())
    results.append(test_paraguayan_search())
    results.append(test_content_analysis())
    
    # Summary
    print("\n" + "=" * 60)
    print("ğŸ“Š Working Webpage Test Results:")
    print("=" * 60)
    
    test_names = [
        "Working Webpages",
        "Financial Data Extraction (Working)", 
        "Paraguayan Content Search",
        "Content Analysis"
    ]
    
    passed = sum(results)
    total = len(results)
    
    for i, (name, result) in enumerate(zip(test_names, results)):
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {status} - {name}")
    
    print(f"\nğŸ“ˆ Overall: {passed}/{total} tests passed ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nğŸ‰ SUCCESS: Web scraping is working with real pages!")
        print("   Ready for full pipeline testing.")
    else:
        print(f"\nâš ï¸  {total-passed} test(s) failed. Check configuration.")
    
    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 